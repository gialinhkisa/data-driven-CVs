{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Deep-LDA CV] Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hpBg5KGjqp6-",
        "gc5aS9mXCqqc",
        "J8Z_5iTY_nTm",
        "FbPMFpDSCFsz",
        "m18wvZQg_u0x",
        "OdE4CWlAS1Yr",
        "rlLw1y9R0mJh",
        "QLa3U64Gyr5R",
        "KpYwhoLkGzdU",
        "5pT8FPdsGzd2",
        "VqoqUEpxAgHY",
        "PrxXGowLGzeA",
        "iSvAhLdZeHBf",
        "U_8bgedTeKcM",
        "GxB5HqczeKcP",
        "J-YznZt_0Hqf",
        "QzGbYHX-0Hqj",
        "Ud5SwKYy0Hqs",
        "j--leyHs0Hq0",
        "yLEWcR0u0Hq-",
        "MvkeMKmB0HrB",
        "BPy0JOH90HrG",
        "YGPqaELn0HrJ",
        "uIYzU-jS0HrM",
        "KRbbqfic8yTq",
        "dT4ZswlG8yUC",
        "Bhh6w75akGRb",
        "el8tiGw_Rs3-",
        "2Rcb2IusAgRi",
        "UZFOf5cv8Om6",
        "iyooOzpf7JiM",
        "h0oCzn4v8yUM",
        "LjF9hUvI4ESz",
        "6PatlEDd8yUQ",
        "1sOOL5upZG0r",
        "Egw88T9tk-EV",
        "4Y5POrHtddwa",
        "iekpwjhOZG1C",
        "c7xjE7cgfuJS",
        "UT51npndD1mz",
        "Li-UQ5wu8yUU",
        "QAbwFHhJ8yUY",
        "eRIkatma8yUc",
        "qrvTAYob8yUg",
        "J1WWc3Hj8yUi",
        "lRk0l699yxhU",
        "dIUsSjCTFFos",
        "0XWHE2Yyj6Ng",
        "ZKXo6EL-rLRX",
        "OWgHfU1sLAQp",
        "JsLqRecHGpKc",
        "XjU8FW7AW1t0",
        "Dwav6dtBAYHx",
        "kYigkC1thrAZ",
        "e5uzJyJKhrAw",
        "0PegXdrXhrA4",
        "y6Ox46eTfDfK",
        "tD9AnFmfhrA-",
        "87TLL5PghrBE",
        "4bjo8CuxhrBI",
        "gyrFnobgaCvC",
        "_5iGmT4Prj3q",
        "sZjdSmc2_cBl",
        "R95FsTP1GW7l",
        "bpcLcfgm_eEQ",
        "sGb10DCR_iwW",
        "vS5UW8sD_kQt",
        "NL28tJ1_6mal",
        "Jnygy_dQrtPE",
        "QkgmTCcBCiEb",
        "5E-2s2DZob0z",
        "JZXnDZzmCZBw",
        "CbOmtoIyCaMq",
        "Z0GwfSo5ITyd",
        "98IadJTJUKBJ",
        "6b0l2nm_pvhJ",
        "NQWuGCwcrtPz",
        "S0JjX2KPEMtB",
        "uvWDvkd-A9Sl",
        "xAJKmezu_Q1a",
        "6xFJxUcaBdbV",
        "Tn-qeP57pnUf",
        "59aDvx2QEUGO",
        "mYlIuE-aAnKa",
        "oYox1YFHzi5c",
        "BSBsU77EaeLY",
        "xxy7lr90anBP",
        "cU4zTG3-bxMH",
        "JERty12gb6E8",
        "P3UqWHr_tuDY",
        "SDI5lFm-tuDc",
        "BvAXa4-htuDq",
        "gl7nSfMrtuD4",
        "orC7IlRLtuD-",
        "ZmG_8VCp8zjO",
        "-qnu0E3stuEC",
        "pr0LCGlFTE8u",
        "HZArjFSLTE80",
        "cl04e0LkTE82",
        "t9_aT3d8TE85",
        "I_zwriisTE88",
        "yDkerjTdqEEy",
        "ja4RFVzdqEE1",
        "tj3oOJ2uQ0m6",
        "jFbM757CQ0m_",
        "3Jm0QF2B7oEs",
        "rpzHvlPGeWv2",
        "hXmUqJ1GpZFe",
        "38cFqN0ppZFf",
        "x-bV9_w9pZFm",
        "sbwzgjLuloc4",
        "1inmmw-bpZFs",
        "46_Pi7GTpZF5",
        "EY-ftzc6B6Mv",
        "ojeiK0Jg9rkR",
        "9BfjANwcvRQ8",
        "ds6D1xxxvVDo",
        "MPuXelc9T_O1",
        "VwzP0yVNzsMt",
        "ZV4sT-rXCEd4",
        "myDIH5ERnc0l",
        "epZDhmVDe1Zm",
        "m_o-hTij1sYt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fp9DSJl6N7b",
        "colab_type": "text"
      },
      "source": [
        "# *Data-driven collective variables for enhanced sampling* - **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uTeBdJ57JJW",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains the code used in the paper \"Data-driven collective variables for enhanced sampling\" by Bonati, Rizzi and Parrinello (2019)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQPAlqi09qBf",
        "colab_type": "text"
      },
      "source": [
        "# Setup and methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdZmI3Ah_Mdb",
        "colab_type": "text"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fRAjy-Duxtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch==1.1.0 torchvision==0.3.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-RQmI_gu6zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_xEHuQy-wU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import progressbar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgmTsqka_c3O",
        "colab_type": "text"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "673FBeOU_bpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ColvarDataset(Dataset):\n",
        "    \"\"\"COLVAR dataset\"\"\"\n",
        "\n",
        "    def __init__(self, colvar_list):\n",
        "        self.nstates = len( colvar_list )\n",
        "        self.colvar = colvar_list\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.colvar[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = ()\n",
        "        for i in range(self.nstates):\n",
        "            x += (self.colvar[i][idx],)\n",
        "        return x\n",
        "    \n",
        "#useful for cycling over the test dataset even if it is smaller than the training set\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05wwc1jT8LT",
        "colab_type": "text"
      },
      "source": [
        "## NN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftykQyXonByN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################\n",
        "# Define Networks\n",
        "##################################\n",
        "\n",
        "class NN_DeepLDA(nn.Module):\n",
        "    \n",
        "    def __init__(self, l ):\n",
        "        super(NN_DeepLDA, self).__init__()\n",
        "        \n",
        "        #Encoder architecture\n",
        "        modules=[]\n",
        "        for i in range( len(l)-1 ):\n",
        "            print(l[i],' --> ', l[i+1], end=' ')\n",
        "            if( i<len(l)-2 ):\n",
        "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
        "                modules.append( nn.ReLU(True) )\n",
        "                print(\"(relu)\")\n",
        "            else:\n",
        "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
        "                print(\"\")\n",
        "                \n",
        "        self.nn = nn.Sequential(*modules)\n",
        "        \n",
        "        #norm option\n",
        "        self.normIn = False\n",
        "        \n",
        "    def set_norm(self, Mean: torch.Tensor, Range: torch.Tensor):\n",
        "        self.normIn = True\n",
        "        self.Mean = Mean\n",
        "        self.Range = Range\n",
        "        \n",
        "    def normalize(self, x: Variable):\n",
        "        batch_size = x.size(0)\n",
        "        x_size = x.size(1)\n",
        "        \n",
        "        Mean = self.Mean.unsqueeze(0).expand(batch_size, x_size)\n",
        "        Range = self.Range.unsqueeze(0).expand(batch_size, x_size)\n",
        "        \n",
        "        return x.sub(Mean).div(Range)\n",
        "    \n",
        "    def get_hidden(self, x: Variable, svd=False, svd_vectors=False, svd_eigen=False, training=False) -> (Variable):\n",
        "        if(self.normIn):\n",
        "            x = self.normalize(x)   \n",
        "        z = self.nn(x)\n",
        "        return z\n",
        "\n",
        "    def set_lda(self, x: torch.Tensor):\n",
        "        self.lda = nn.Parameter(x.unsqueeze(0), requires_grad=False) \n",
        "\n",
        "    def get_lda(self) -> (torch.Tensor):\n",
        "        return self.lda\n",
        "\n",
        "    def apply_lda(self, x: Variable) -> (Variable):\n",
        "        z = torch.nn.functional.linear(x,self.lda)\n",
        "        return z\n",
        "        \n",
        "    def forward(self, x: Variable) -> (Variable):\n",
        "        z = self.get_hidden(x,svd=False)\n",
        "        z = self.apply_lda(z)\n",
        "        return z\n",
        "\n",
        "    def get_cv(self, x: Variable) -> (Variable):\n",
        "        return self.forward(x)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxWfXAPNHTzt",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJGjY4SGUPKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- loss function --\n",
        "def LDAloss_cholesky(H, label, test_routines=False):\n",
        "    #sizes\n",
        "    N, d = H.shape\n",
        "    \n",
        "    #H = H*1e2\n",
        "\n",
        "    # Mean centered observations for entire population\n",
        "    H_bar = H - torch.mean(H, 0, True)\n",
        "    #Total scatter matrix (cov matrix over all observations)\n",
        "    S_t = H_bar.t().matmul(H_bar) / (N - 1)\n",
        "    #Define within scatter matrix and compute it\n",
        "    S_w = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)    \n",
        "    S_w_inv = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)\n",
        "    buf = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)\n",
        "    #Loop over classes to compute means and covs\n",
        "    for i in range(categ):\n",
        "        #check which elements belong to class i\n",
        "        H_i = H[torch.nonzero(label == i).view(-1)]\n",
        "        # compute mean centered obs of class i\n",
        "        H_i_bar = H_i - torch.mean(H_i, 0, True)\n",
        "        # count number of elements\n",
        "        N_i = H_i.shape[0]\n",
        "        if N_i == 0:\n",
        "            continue\n",
        "        \n",
        "        #LDA\n",
        "        S_w += H_i_bar.t().matmul(H_i_bar) / ((N_i - 1) * categ)\n",
        "        \n",
        "        ######HLDA\n",
        "        #inv_i = H_i_bar.t().matmul(H_i_bar) / ((N_i - 1) * categ)\n",
        "        #S_w_inv += inv_i.pinverse()       \n",
        "        \n",
        "    #S_w = S_w_inv.pinverse()\n",
        "    #END HLDA#########        \n",
        "\n",
        "    S_b = S_t - S_w\n",
        "\n",
        "    S_w = S_w + lambdA * torch.diag(torch.Tensor().new_ones((d), device = device, dtype = dtype))\n",
        "\n",
        "    ## Generalized eigenvalue problem: S_b * v_i = lambda_i * Sw * v_i \n",
        "\n",
        "    # (1) use cholesky decomposition for S_w\n",
        "    L = torch.cholesky(S_w,upper=False)\n",
        "\n",
        "    # (2) define new matrix using cholesky decomposition and \n",
        "    L_t = torch.t(L)\n",
        "    L_ti = torch.inverse(L_t)\n",
        "    L_i = torch.inverse(L)\n",
        "    S_new = torch.matmul(torch.matmul(L_i,S_b),L_ti)\n",
        "\n",
        "    # (3) solve  S_new * w_i = lambda_i * w_i\n",
        "    eig_values, eig_vectors = torch.symeig(S_new,eigenvectors=True)\n",
        "    eig_vectors = eig_vectors.t()\n",
        "    # (4) sort eigenvalues and retrieve old eigenvector \n",
        "    #eig_values, ind = torch.sort(eig_values, 0, descending=True)\n",
        "    max_eig_vector = eig_vectors[-1]   \n",
        "    max_eig_vector = torch.matmul(L_ti,max_eig_vector)\n",
        "    norm=max_eig_vector.pow(2).sum().sqrt()\n",
        "    max_eig_vector.div_(norm)\n",
        "\n",
        "    loss = - eig_values[-1]\n",
        "\n",
        "    return loss, eig_values, max_eig_vector, S_b, S_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meatQIj6gj4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_LDA_cholesky(loader, model):\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            X,y = data[0].float().to(device),data[1].long().to(device)\n",
        "            H  = model.get_hidden(X)\n",
        "            _, eig_values, eig_vector, _, _ = LDAloss_cholesky(H, y)\n",
        "    return eig_values, eig_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5W2-rzUGzdt"
      },
      "source": [
        "## Encode (for analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZbVufwoBRdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_hidden(loader,model,batch,n_hidden,device):\n",
        "    \"\"\"Compute the compressed representation for an entire dataset (with two classes A and B)\"\"\"\n",
        "    s=np.empty((len(loader),batch,n_hidden))\n",
        "    l=np.empty((len(loader),batch))\n",
        "    for i,data in enumerate(loader):\n",
        "        x,lab = data[0].float(),data[1].long()\n",
        "        x = Variable(x).to(device)\n",
        "        cv = model.get_hidden(x,svd=False)\n",
        "        #cv = model.apply_pca(cv)\n",
        "        s[i] = cv.detach().cpu().numpy()\n",
        "        l[i] = lab\n",
        "        \n",
        "    s=s.reshape(len(loader)*batch,n_hidden)\n",
        "    s=s[0:len(loader)*batch]\n",
        "\n",
        "    l=l.reshape(len(loader)*batch)\n",
        "    l=l[0:len(loader)*batch]\n",
        "    \n",
        "    sA = s[l==0]\n",
        "    sB = s[l==1]\n",
        "\n",
        "    return sA,sB\n",
        "\n",
        "def encode_cv(loader,model,batch,n_cv,device):\n",
        "    \"\"\"Compute the compressed representation for an entire dataset (with two classes A and B)\"\"\"\n",
        "    s=np.empty((len(loader),batch,n_cv))\n",
        "    l=np.empty((len(loader),batch))\n",
        "    for i,data in enumerate(loader):\n",
        "        x,lab = data[0].float(),data[1].long()\n",
        "        x = Variable(x).to(device)\n",
        "        cv = model(x)\n",
        "        s[i] = cv.detach().cpu().numpy()\n",
        "        l[i] = lab\n",
        "        \n",
        "    s=s.reshape(len(loader)*batch,n_cv)\n",
        "    s=s[0:len(loader)*batch]\n",
        "\n",
        "    l=l.reshape(len(loader)*batch)\n",
        "    l=l[0:len(loader)*batch]\n",
        "    \n",
        "    sA = s[l==0]\n",
        "    sB = s[l==1]\n",
        "\n",
        "    return sA,sB\n",
        "\n",
        "def encode_cv_all(loader,model,batch,n_cv,device):\n",
        "    \"\"\"Compute the compressed representation for an entire dataset (with two classes A and B)\"\"\"\n",
        "    s=np.empty((len(loader),batch,n_cv))\n",
        "    l=np.empty((len(loader),batch))\n",
        "    for i,data in enumerate(loader):\n",
        "        x,lab = data[0].float(),data[1].long()\n",
        "        x = Variable(x).to(device)\n",
        "        cv = model.get_cv(x)\n",
        "        s[i] = cv.detach().cpu().numpy()\n",
        "        l[i] = lab\n",
        "        \n",
        "    s=s.reshape(len(loader)*batch,n_cv)\n",
        "    s=s[0:len(loader)*batch]\n",
        "\n",
        "    l=l.reshape(len(loader)*batch)\n",
        "    l=l[0:len(loader)*batch]\n",
        "\n",
        "    return s,l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PYdasaHDIzqn"
      },
      "source": [
        "## Plot functions with save opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SlqsSwjXIzqr",
        "colab": {}
      },
      "source": [
        "def plot_results(save=False,testing=False,accuracy=False,chem_space=False):\n",
        "    ngrid=3\n",
        "    if accuracy:\n",
        "        ngrid=4\n",
        "        if chem_space:\n",
        "            ngrid=5\n",
        "    for i in range(ngrid):\n",
        "        with grid.output_to(0,i):\n",
        "            grid.clear_cell()\n",
        "    with grid.output_to(0,0):\n",
        "        plot_training(save)\n",
        "    with grid.output_to(0,1):\n",
        "        plot_H(save,testing)\n",
        "    with grid.output_to(0,2):\n",
        "        plot_CV(save,testing)\n",
        "    if accuracy:\n",
        "        with grid.output_to(0,3):\n",
        "            plot_accuracy(save,testing)\n",
        "        if chem_space:\n",
        "            with grid.output_to(0,4):\n",
        "                plot_chem_space(save)\n",
        "\n",
        "def plot_training(save=False):\n",
        "    pylab.figure(figsize=(5, 5))\n",
        "    pylab.title(\"Deep-LDA optimization\")\n",
        "    pylab.plot(np.asarray(ep),np.asarray(eig),'.-', c='tab:green', label='batch')\n",
        "    pylab.plot(np.asarray(ep),np.asarray(eig_t),'.-', c='tab:grey', label='population')\n",
        "    pylab.xlabel(\"Epoch\")\n",
        "    pylab.ylabel(\"1st Eigenvalue\")\n",
        "    pylab.legend()\n",
        "    if save:\n",
        "        pylab.savefig(\"{}/{}.png\".format(tr_folder, \"training\"),dpi=150)\n",
        "\n",
        "def plot_accuracy(save=False,testing=False):\n",
        "    pylab.figure(figsize=(5, 5))\n",
        "    pylab.title(\"Classification accuracy\")\n",
        "    pylab.plot(np.asarray(ep),100*np.asarray(acc),'.-', c='tab:cyan', label='training')\n",
        "    if testing:\n",
        "        pylab.plot(np.asarray(ep),100*np.asarray(acc_t),'.-', c='tab:orange', label='testing')\n",
        "    pylab.xlabel(\"Epoch\")\n",
        "    pylab.ylabel(\"Accuracy (%)\")\n",
        "    pylab.legend()\n",
        "    if save:\n",
        "        pylab.savefig(\"{}/{}.png\".format(tr_folder, \"accuracy\"),dpi=150)\n",
        "\n",
        "def plot_H(save=False,testing=False):\n",
        "    pylab.figure(figsize=(5, 5))\n",
        "    pylab.title(\"LDA on Hidden-space H\")\n",
        "    # -- Testing and Validation histograms --\n",
        "    trA,trB = encode_hidden(valid_loader_labels,model,batch_val,n_hidden,device)\n",
        "    eigen=max_eig_vector.detach().numpy()\n",
        "\n",
        "    pylab.scatter(trA[:,0],trA[:,1], c='tab:red', label='trA',alpha=0.3)\n",
        "    pylab.scatter(trB[:,0],trB[:,1], c='tab:blue', label='trB',alpha=0.3)\n",
        "\n",
        "    if testing:\n",
        "        ttA,ttB = encode_hidden(test_meta_labels,model,batch_test,n_hidden,device)\n",
        "        pylab.scatter(ttA[:,0],ttA[:,1], c='tab:orange', label='testA',s=0.2, alpha=0.5)\n",
        "        pylab.scatter(ttB[:,0],ttB[:,1], c='tab:cyan', label='testB',s=0.2, alpha=0.5)\n",
        "        mIN=np.min([np.min(trA[:,0]),np.min(trB[:,0]),np.min(ttA[:,0]),np.min(ttB[:,0])])\n",
        "        mAX=np.max([np.max(trA[:,0]),np.max(trB[:,0]),np.max(ttA[:,0]),np.max(ttB[:,0])])\n",
        "    else:\n",
        "        mIN=np.min([np.min(trA[:,0]),np.min(trB[:,0])])\n",
        "        mAX=np.max([np.max(trA[:,0]),np.max(trB[:,0])])\n",
        "\n",
        "    x=np.linspace(mIN,mAX,100)\n",
        "    y=-eigen[0]/eigen[1]*x+0\n",
        "    #pylab.plot(x,y, linewidth=2, label='DeepLDA')\n",
        "    pylab.legend() \n",
        "    if save:\n",
        "        pylab.savefig(\"{}/{}.png\".format(tr_folder, \"hidden\"),dpi=150)\n",
        "\n",
        "def plot_CV(save=False,testing=False):\n",
        "    sA,sB = encode_cv(valid_loader_labels,model,batch_val,n_cv,device)\n",
        "    sA,sB = sA[:,0], sB[:,0]\n",
        "    if testing:\n",
        "        stA,stB = encode_cv(test_meta_labels,model,batch_test,n_cv,device)\n",
        "        stA,stB = stA[:,0], stB[:,0]\n",
        "\n",
        "        min_s=np.min([np.min(sA),np.min(sB),np.min(stA),np.min(stB)])\n",
        "        max_s=np.max([np.max(sA),np.max(sB),np.max(stA),np.max(stB)])\n",
        "    else:\n",
        "        min_s=np.min([np.min(sA),np.min(sB)])\n",
        "        max_s=np.max([np.max(sA),np.max(sB)])\n",
        "    \n",
        "    b=np.linspace(min_s,max_s,100)\n",
        "\n",
        "    pylab.figure(figsize=(5, 5))\n",
        "    pylab.title(\"Deep-LDA CV Histogram\")\n",
        "    pylab.hist(sA, bins=b, ls='dashed', alpha = 0.7, lw=2, color='tab:red', label='trA',density=True)\n",
        "    pylab.hist(sB, bins=b, ls='dashed', alpha = 0.7, lw=2, color='tab:blue', label='trB',density=True)\n",
        "\n",
        "    if testing:\n",
        "        pylab.hist(stA, bins=b, ls='dashed', alpha = 0.5, lw=2, color='tab:orange', label='testA',density=True)\n",
        "        pylab.hist(stB, bins=b, ls='dashed', alpha = 0.5, lw=2, color='tab:cyan', label='testB',density=True)\n",
        "\n",
        "    pylab.legend()\n",
        "    if save:\n",
        "        pylab.savefig(\"{}/{}.png\".format(tr_folder, \"histogram\"), dpi=150)\n",
        "\n",
        "def plot_chem_space(save):\n",
        "    s,l = encode_cv_all(test_meta_ord_labels,model,batch_test,n_cv,device)\n",
        "    s = s[:,0]\n",
        "\n",
        "    x = de_cc\n",
        "    y = de_oh1\n",
        "\n",
        "    x = x[:len(s)]\n",
        "    y = y[:len(s)]\n",
        "\n",
        "    pylab.figure(figsize=(5,5))\n",
        "    scat = pylab.scatter(x,y,c=s,cmap=cm_fessa,s=1.)\n",
        "    pylab.colorbar(scat)\n",
        "    if save:\n",
        "        pylab.savefig(\"{}/{}.png\".format(tr_folder, \"chem_space\"), dpi=150)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KRbbqfic8yTq"
      },
      "source": [
        "# Ala2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhh6w75akGRb",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_5xbf3mkFHz",
        "colab": {}
      },
      "source": [
        "folder=main_folder+\"ala2/unbiased/\"\n",
        "\n",
        "n_dist=45\n",
        "n_input=n_dist\n",
        "\n",
        "distA=np.loadtxt(folderA+\"INPUTS.A\",usecols=range(1,n_dist+1))\n",
        "distB=np.loadtxt(folderB+\"INPUTS.B\",usecols=range(1,n_dist+1))   \n",
        "\n",
        "print(distA.shape)\n",
        "\n",
        "if normalize:\n",
        "    # normalize inputs\n",
        "    Max=np.amax(np.concatenate([distA,distB],axis=0),axis=0)\n",
        "    Min=np.amin(np.concatenate([distA,distB],axis=0),axis=0)\n",
        "\n",
        "    Mean=(Max+Min)/2.\n",
        "    Range=(Max-Min)/2.\n",
        "    Range[Range<1e-6]=1.\n",
        "\n",
        "    if all_input:\n",
        "        #do not normalize angles\n",
        "        Mean[n_dist:]=0.\n",
        "        Range[n_dist:]=1.\n",
        "\n",
        "# create labels\n",
        "lA=np.zeros_like(distA[:,0])\n",
        "lB=np.ones_like(distB[:,0])\n",
        "\n",
        "dist=np.concatenate([distA,distB],axis=0)\n",
        "dist_label=np.concatenate([lA,lB],axis=0)\n",
        "\n",
        "p = np.random.permutation(len(dist))\n",
        "dist, dist_label = dist[p], dist_label[p]\n",
        "\n",
        "#assign equal weights for testing\n",
        "w=np.ones_like(dist_label)\n",
        "\n",
        "train_data=20000\n",
        "batch_tr=2000\n",
        "train_labels=ColvarDataset([dist[:train_data],dist_label[:train_data],w[:train_data]])\n",
        "train_loader_labels=DataLoader(train_labels, batch_size=batch_tr,shuffle=True)\n",
        "\n",
        "valid_data=20000\n",
        "batch_val=20000\n",
        "valid_labels=ColvarDataset([dist[:valid_data],dist_label[:valid_data],w[:valid_data]])\n",
        "#valid_labels=ColvarDataset([dist[train_data:train_data+test_data],dist_label[train_data:train_data+test_data]])\n",
        "valid_loader_labels=DataLoader(valid_labels, batch_size=batch_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y5POrHtddwa",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5FjH9AdhMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type\n",
        "dtype = torch.float32\n",
        "\n",
        "#parameters\n",
        "categ = 2\n",
        "eig_num = 1\n",
        "\n",
        "# wheter to use CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_input=n_dist\n",
        "n_hidden=5\n",
        "n_cv=1\n",
        "nodes=[n_input,30,15,n_hidden]\n",
        "normalize = True\n",
        "\n",
        "print(\"===== NN =====\")\n",
        "model = NN_DeepLDA(nodes)\n",
        "if normalize:\n",
        "    model.set_norm(torch.tensor(Mean,dtype=dtype,device=device),torch.tensor(Range,dtype=dtype,device=device))\n",
        "print(\"======================\")\n",
        "model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"using CUDA acceleration\")\n",
        "    print(\"========================\")\n",
        "\n",
        "# -- Optimization --\n",
        "lambdA=0.05\n",
        "lrate = 0.0001\n",
        "l2_reg = 1e-5\n",
        "act_reg = 2./lambdA\n",
        "\n",
        "num_epochs=50\n",
        "print_ep=5\n",
        "\n",
        "#define arrays and values\n",
        "ep = []\n",
        "eig = []\n",
        "eig_t = []\n",
        "acc = []\n",
        "acc_t = []\n",
        "init_epoch = 0\n",
        "best_result = 0\n",
        "best_value = 0\n",
        "best_vectors = 0\n",
        "\n",
        "#OPTIMIZERS\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
        "\n",
        "#format output\n",
        "float_formatter = lambda x: \"%.6f\" % x\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
        "\n",
        "# grid settings --> ONLY FOR COLAB\n",
        "#from matplotlib import pylab\n",
        "#from google.colab import widgets\n",
        "\n",
        "#grid = widgets.Grid(1,4)\n",
        "\n",
        "print('[{:>3}/{:>3}] {:>10} {:>10} {:>10} {:>10} {:>10}'.format('ep','tot','eig_tr','eig_test','reg'))\n",
        "\n",
        "# -- Training --\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader_labels:\n",
        "        # =================get data===================\n",
        "        X,y = data[0].float().to(device),data[1].long().to(device)\n",
        "        # =================forward====================\n",
        "        H = model.get_hidden(X)\n",
        "        # =================reg loss===================\n",
        "        reg_loss = s.pow(2).sum().div( s.size(0) )\n",
        "        reg_loss_lor = - act_reg / (1+(reg_loss-1).pow(2))\n",
        "        # =================backprop===================\n",
        "        opt.zero_grad()\n",
        "        lossg.backward(retain_graph=True)\n",
        "        reg_loss_lor.backward()\n",
        "        opt.step()\n",
        "\n",
        "    #Compute LDA over entire dataset\n",
        "    test_eig_values, test_eig_vector = check_LDA_cholesky(valid_loader_labels, model)\n",
        "    model.set_lda(test_eig_vector)    \n",
        "\n",
        "    #Compute accuracy\n",
        "    accu_train = classify(valid_loader_labels,train_loader_labels,model)\n",
        "    #accu_test = classify(valid_loader_labels,test_meta_labels,model)\n",
        "    \n",
        "    #save results\n",
        "    ep.append(epoch+init_epoch+1)\n",
        "    eig.append(eig_values[-1])\n",
        "    eig_t.append(test_eig_values[-1])\n",
        "    acc.append(accu_train)\n",
        "    #acc_t.append(accu_test)\n",
        "    print\n",
        "    if (epoch+1)%1 == 0:\n",
        "        print('[{:3d}/{:3d}] {:10.2f} {:10.2f} {:10.2f} {:10.2G} {:10.2G}'.format\n",
        "          (init_epoch+epoch+1, init_epoch+num_epochs, eig_values.detach().numpy()[-1], test_eig_values.numpy()[-1], reg_loss, test_eig_vector.numpy())\n",
        "\n",
        "    if (epoch+1)%print_ep == 0:\n",
        "        plot_results(testing=True, accuracy=True)\n",
        "\n",
        "    if test_eig_values[0] > best_result:\n",
        "        best_result = test_eig_values[0]\n",
        "        best_value = test_eig_values\n",
        "        best_vectors = test_eig_vector\n",
        "        #torch.save(model, \"model_DeepLDA.pt\")\n",
        "        \n",
        "print(\"--------------\")\n",
        "print(\"-- Eigenvalues [Last // Best] --\")\n",
        "print(test_eig_values,best_value)\n",
        "print(\"-- LDA Eigenvector [Last // Best]--\")\n",
        "print(test_eig_vector,best_vectors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44K3HkpW-vQK",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze hidden space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXR6dg7EiwPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trA,trB = encode_hidden(valid_loader_labels,model,batch_val,n_hidden,device)\n",
        "\n",
        "print(trA.shape)\n",
        "\n",
        "for i in range(trA.shape[1]):\n",
        "    pylab.figure(figsize=(5, 5))\n",
        "    pylab.title(\"h_\"+str(i))\n",
        "    pylab.plot(trA[:,i], c='tab:red', label='trA',alpha=0.7)\n",
        "    pylab.plot(trB[:,i], c='tab:blue', label='trB',alpha=0.7)\n",
        "    pylab.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLM9ahz6MRdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(trA.shape[1]):\n",
        "    for j in range(i+1,trA.shape[1]):\n",
        "        pylab.figure(figsize=(5, 5))\n",
        "        pylab.title(\"h_\"+str(i)+\" vs h_\"+str(j))\n",
        "        pylab.scatter(trA[:,i],trA[:,j], c='tab:red', label='trA',alpha=0.7)\n",
        "        pylab.scatter(trB[:,i],trB[:,j], c='tab:blue', label='trB',alpha=0.7)\n",
        "        pylab.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNTuN1vY-4WN",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH71QJQwmIf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trA,trB = encode_cv(valid_loader_labels,model,batch_val,n_cv,device)\n",
        "trA,trB = encode_cv(valid_loader_labels,model,batch_val,1,'cpu')\n",
        "\n",
        "a = trA[:,0]\n",
        "b = trB[:,0]\n",
        "\n",
        "print(\"==A==\")\n",
        "print(np.mean(a),np.std(a))\n",
        "print(np.amin(a),np.amax(a))\n",
        "\n",
        "print(\"==B==\")\n",
        "print(np.mean(b),np.std(b))\n",
        "print(np.amin(b),np.amax(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iekpwjhOZG1C"
      },
      "source": [
        "### Export model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DfkudsnGZG1E",
        "colab": {}
      },
      "source": [
        "# == Set output folder\n",
        "tr_folder=main_folder+\"ala2/\"\n",
        "!mkdir -p \"{tr_folder}\"\n",
        "\n",
        "# == Plot and save results == \n",
        "grid = widgets.Grid(1,3)\n",
        "plot_results(save=True,testing=True)\n",
        "\n",
        "# == Create fake dataloader ==\n",
        "fake_loader = DataLoader(train_labels, batch_size=1,shuffle=False)\n",
        "fake_input = next(iter(fake_loader ))[0].float()\n",
        "\n",
        "# == Export model ==\n",
        "mod = torch.jit.trace(model, fake_input)\n",
        "mod.save(tr_folder+\"model.pt\")\n",
        "print(\"@@ exported model in: \",tr_folder+\"model.pt\" )\n",
        "\n",
        "# == SAVE LDA COEFFICIENTS ==\n",
        "f = open(tr_folder+\"lda.dat\", \"w\")\n",
        "f.write(str(model.get_lda().numpy()))\n",
        "f.close()\n",
        "\n",
        "# == EXPORT CHECKPOINT ==\n",
        "torch.save({\n",
        "            'epoch': num_epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            }, tr_folder+\"checkpoint\")\n",
        "\n",
        "print(\"@@ checkpoint: \",tr_folder+\"checkpoint\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnu0hWZhDWQg",
        "colab_type": "text"
      },
      "source": [
        "# Aldol reaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snvTmURcDeCu",
        "colab_type": "text"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6uKFoFADgBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder=main_folder+\"aldol/unbiased/\"\n",
        "\n",
        "normalize=False\n",
        "\n",
        "n_input=40\n",
        "\n",
        "# -- Loading and preprocessing --\n",
        "distA=np.loadtxt(folder+\"INPUTS.R\",usecols=range(1,n_input+1))\n",
        "distB=np.loadtxt(folder+\"INPUTS.P\",usecols=range(1,n_input+1))\n",
        "\n",
        "print(\"A\",distA.shape)\n",
        "print(\"B\",distB.shape)\n",
        "\n",
        "if normalize:\n",
        "    # normalize inputs\n",
        "    Max=np.amax(np.concatenate([distA,distB],axis=0),axis=0)\n",
        "    Min=np.amin(np.concatenate([distA,distB],axis=0),axis=0)\n",
        "\n",
        "    Mean=(Max+Min)/2.\n",
        "    Range=(Max-Min)/2.\n",
        "    Range[Range<1e-6]=1.\n",
        "\n",
        "# create labels\n",
        "lA=np.zeros_like(distA[:,0])\n",
        "lB=np.ones_like(distB[:,0])\n",
        "\n",
        "dist=np.concatenate([distA,distB],axis=0)\n",
        "dist_label=np.concatenate([lA,lB],axis=0)\n",
        "\n",
        "p = np.random.permutation(len(dist))\n",
        "dist, dist_label = dist[p], dist_label[p]\n",
        "\n",
        "#assign equal weights for testing\n",
        "w=np.ones_like(dist_label)\n",
        "\n",
        "train_data=10000\n",
        "batch_tr=2000\n",
        "train_labels=ColvarDataset([dist[:train_data],dist_label[:train_data],w[:train_data]])\n",
        "train_loader_labels=DataLoader(train_labels, batch_size=batch_tr,shuffle=True)\n",
        "\n",
        "valid_data=train_data\n",
        "batch_val=train_data\n",
        "valid_labels=ColvarDataset([dist[:valid_data],dist_label[:valid_data],w[:valid_data]])\n",
        "valid_loader_labels=DataLoader(valid_labels, batch_size=batch_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EHTaBXGD6Ye",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy5kwySzD5lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#type\n",
        "dtype = torch.float32\n",
        "\n",
        "#parameters\n",
        "categ = 2\n",
        "eig_num = 1\n",
        "\n",
        "# wheter to use CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_hidden=3\n",
        "n_cv=1\n",
        "nodes=[n_input,20,10,n_hidden]\n",
        "normalize = False\n",
        "\n",
        "print(\"===== NN =====\")\n",
        "model = NN_DeepLDA(nodes)\n",
        "if normalize:\n",
        "    model.set_norm(torch.tensor(Mean,dtype=dtype,device=device),torch.tensor(Range,dtype=dtype,device=device))\n",
        "print(\"======================\")\n",
        "model.to(device)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"using CUDA acceleration\")\n",
        "    print(\"========================\")\n",
        "\n",
        "# -- Optimization --\n",
        "lambdA = 0.05\n",
        "lrate = 0.0001\n",
        "l2_reg = 1e-5\n",
        "act_reg = 2./lambdA\n",
        "\n",
        "num_epochs=50\n",
        "print_ep=5\n",
        "\n",
        "#define arrays and values\n",
        "ep = []\n",
        "eig = []\n",
        "eig_t = []\n",
        "acc = []\n",
        "acc_t = []\n",
        "init_epoch = 0\n",
        "best_result = 0\n",
        "\n",
        "#OPTIMIZERS\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
        "\n",
        "#format output\n",
        "float_formatter = lambda x: \"%.6f\" % x\n",
        "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
        "\n",
        "# grid settings --> ONLY FOR COLAB\n",
        "#from matplotlib import pylab\n",
        "#from google.colab import widgets\n",
        "\n",
        "#grid = widgets.Grid(1,5)\n",
        "\n",
        "print('[{:>3}/{:>3}] {:>10} {:>10} {:>10} {:>10} {:>10}'.format('ep','tot','eig_tr','eig_test','reg'))\n",
        "\n",
        "# -- Training --\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader_labels:\n",
        "        # =================get data===================\n",
        "        X,y = data[0].float().to(device),data[1].long().to(device)\n",
        "        # =================forward====================\n",
        "        H = model.get_hidden(X)\n",
        "        # =================lda loss===================\n",
        "        lossg, eig_values, max_eig_vector, Sb, Sw = LDAloss_cholesky(H, y)\n",
        "        # =================reg loss===================\n",
        "        reg_loss = s.pow(2).sum().div( s.size(0) )\n",
        "        reg_loss_lor = - act_reg / (1+(reg_loss-1).pow(2))\n",
        "        # =================backprop===================\n",
        "        opt.zero_grad()\n",
        "        lossg.backward(retain_graph=True)\n",
        "        reg_loss_lor.backward()\n",
        "        opt.step()\n",
        "        \n",
        "    \n",
        "    #Compute LDA over entire dataset\n",
        "    test_eig_values, test_eig_vector = check_LDA_cholesky(valid_loader_labels, model)\n",
        "    model.set_lda(test_eig_vector)    \n",
        "    #Compute accuracy\n",
        "    accu_train = classify(valid_loader_labels,train_loader_labels,model)\n",
        "    #accu_test = classify(valid_loader_labels,test_meta_labels,model)\n",
        "    #save results\n",
        "    ep.append(epoch+init_epoch+1)\n",
        "    eig.append(eig_values[-1])\n",
        "    #eig_t.append(test_eig_values[-1])\n",
        "    acc.append(accu_train)\n",
        "    #acc_t.append(accu_test)\n",
        "    print\n",
        "    if (epoch+1)%1 == 0:\n",
        "        print('[{:3d}/{:3d}] {:10.2f} {:10.2f} {:10.2f} {:10.2G} {:10.2G}'.format\n",
        "          (init_epoch+epoch+1, init_epoch+num_epochs, eig_values.detach().numpy()[-1], test_eig_values.numpy()[-1], reg_loss), test_eig_vector.numpy() )\n",
        "\n",
        "    if (epoch+1)%print_ep == 0:\n",
        "        #grid = widgets.Grid(1,5)\n",
        "        #plot_results(testing=True, accuracy=True,chem_space=True)\n",
        "        #print(Sw)\n",
        "\n",
        "    if test_eig_values[0] > best_result:\n",
        "        best_result = test_eig_values[0]\n",
        "        best_value = test_eig_values\n",
        "        best_vectors = test_eig_vector\n",
        "        #torch.save(model, \"model_DeepLDA.pt\")\n",
        "        \n",
        "print(\"--------------\")\n",
        "print(\"-- Eigenvalues [Last // Best] --\")\n",
        "print(test_eig_values,best_value)\n",
        "print(\"-- LDA Eigenvector [Last // Best]--\")\n",
        "print(test_eig_vector,best_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeqK1FKIN-Y0",
        "colab_type": "text"
      },
      "source": [
        "#### Export model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FddZGBCSOAwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# == Set output folder\n",
        "tr_folder=main_folder+\"aldol/\"\n",
        "!mkdir -p \"{tr_folder}\"\n",
        "\n",
        "# == Plot and save results == \n",
        "grid = widgets.Grid(1,5)\n",
        "plot_results(save=True,testing=True,accuracy=True,chem_space=True)\n",
        "\n",
        "# == Create fake dataloader ==\n",
        "fake_loader = DataLoader(train_labels, batch_size=1,shuffle=False)\n",
        "fake_input = next(iter(fake_loader ))[0].float()\n",
        "\n",
        "# == Export model ==\n",
        "mod = torch.jit.trace(model, fake_input)\n",
        "mod.save(tr_folder+\"model.pt\")\n",
        "print(\"@@ exported model in: \",tr_folder+\"model.pt\" )\n",
        "\n",
        "# == SAVE LDA COEFFICIENTS ==\n",
        "f = open(tr_folder+\"lda.dat\", \"w\")\n",
        "f.write(str(model.get_lda().numpy()))\n",
        "f.close()\n",
        "\n",
        "# == EXPORT CHECKPOINT ==\n",
        "torch.save({\n",
        "            'epoch': num_epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            }, tr_folder+\"checkpoint\")\n",
        "\n",
        "print(\"@@ checkpoint: \",tr_folder+\"checkpoint\" )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}